\
<< Incomplete >>

 The issues with Skewed/Imbalanced classes were also discussed and the use of dummy classifiers, and the use of metrics other than accuracy to measure the score of a classifer. In particular the advantages of recall, precision and F1. Plotting a confusion matrix to visualize to view True and False positives, and True and False Negatives. In certain applications a learning algorithm may be more better suited to only very confidently picking y=1 cases (eg. displaying a particular ad to a user) for precision-oriented, or recall-oriented eg. predicting whether or not someone has cancer - where we would want to investigate all possibilities in more detail and not miss any possible false positives. 

The importance of precision-recall curves, or better still the ROC (Receiver Operator Characteristic) Curve- True postives vs. False positive were explained. A higher AUC under a ROC indicates a more successful algorithm.
